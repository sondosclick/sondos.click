---
layout: post
title: "Epilogue: it’s not fear of AI, it’s respect for complexity"
date: 2026-03-29
categories: [AI, Technology, Culture]
tags: [AI, Complexity, Critical Thinking, Responsibility]
description: "Series closing: it’s not fear of AI, it’s respect for complexity."
featureimage: "/images/posts/2026-03-29-epilogo-respeto-complejidad/feature.svg"
---

# Epilogue: it’s not fear of AI, it’s respect for complexity

![Illustration about navigating complexity](/images/posts/2026-03-29-epilogo-respeto-complejidad/feature.svg)

If you’ve made it this far, there’s something worth clarifying before we go on.

Yes, this text—like the previous ones—was written with the help of AI.  
Specifically, by a black box with a smile that answers quickly, writes well, and never gets tired.

The same box we’ve spent several articles asking for caution about.

The irony isn’t lost on us.  
And, in fact, it reinforces the message.

Because the problem was never using AI.  
The problem was confusing how well it writes with how well it understands,  
and how fast it produces with how well it decides.

Throughout this series we’ve talked about many different things that, underneath, are the same.

We’ve talked about excessive simplification, delegated authority, amplified biases, diluted responsibility, juniors learning to execute without understanding, and metrics that celebrate speed while ignoring cost.

All of those point to the same risk: losing sight of the real complexity of human and technical systems just when we have more tools to hide it.

AI doesn’t eliminate complexity.  
It displaces it.

And when complexity is displaced, someone ends up carrying it.

We’ve also insisted on something that’s worth repeating one last time: this is not a crusade against AI.

AI is extraordinarily useful in many tasks.  
Especially in the ones it was designed for: working with language, frequent patterns, synthesis, rephrasing, and exploring alternatives.

In fact, this series exists because using it to think, write, and contrast ideas works very well.

The problem appears when we extend that trust to domains where decisions have cumulative consequences, errors are discovered late, and responsibility can’t be delegated without redesigning the system.

There, it’s not enough that something works.  
You need to understand why it works, who responds when it stops working, and what we’re sacrificing in exchange.

If we take this reflection outside the professional domain, the questions don’t disappear. They multiply.

What happens to jobs when we automate tasks without rethinking training?  
What happens to the economy when apparent productivity grows faster than real understanding?  
What impact does all of this have on energy consumption and the environmental footprint of systems that scale without friction?  
What social changes are introduced by a technology that answers with authority without understanding context or consequences?

These aren’t technical questions.  
But they aren’t abstract either.

They are questions about how we coexist with systems that increasingly influence human decisions, without being human.

Hopefully, before long, many of these concerns will become obsolete.  
Hopefully, AI will understand better, fail less, and integrate more maturely into our organizations and societies.

There’s no problem with being wrong.

The real problem would be not asking these questions before delegating too much, before redesigning entire structures, and before confusing convenience with progress.

Because critical thinking isn’t a brake on innovation.  
It’s what makes it sustainable.

And no box, no matter how well it writes, can do that for us.

## Conceptual references that appear throughout the series

The topics covered in this series are grounded in ideas and concepts long studied across fields. They aren’t new ideas, even if they now reappear under different names.

- **Cognitive load and human limits in complex tasks**  
  Cognitive load theory applied to learning, problem-solving, and reviewing others’ work.

- **Illusion of understanding**  
  The human tendency to believe we understand a system because we can explain it superficially or because it “works.”

- **Ironies of automation**  
  How automated systems shift humans to less frequent but more critical tasks, increasing risk when they fail.

- **Biases amplified by algorithmic systems**  
  Systems don’t just inherit existing biases; they scale and normalize them.

- **Technical debt and cumulative complexity**  
  Reasonable short-term decisions that generate growing long-term costs.

- **Perverse effects of metrics**  
  When a measure becomes a target, it ceases to be a good measure.

- **Professional learning and deliberate practice**  
  The difference between executing tasks and developing judgment through deep understanding.

These ideas appear in cognitive psychology, software engineering, sociology of technology, and organizational economics for decades.

## To go deeper (videos and talks)

If you want to explore these ideas further, here are some videos and talks that can accompany the reading. This list is deliberately open and revisable (and it doesn’t imply I’ve watched/read them all… yet).

### Cognitive load and intellectual work

- MIT OpenCourseWare – [How People Learn: Cognitive Load Theory](https://www.youtube.com/watch?v=P4RVO9aIIVA)  
- Modern Software Engineering – [What Is The True Cost Of Cognitive Load In Software Engineering?](https://www.youtube.com/watch?v=WBpIY43QU6w)

### Automation, humans in the loop, and control

- Stuart Russell – [3 principles for creating safer AI (TED)](https://www.youtube.com/watch?v=EBK-a94IFHY)

### Bias, algorithms, and automated decisions

- Cathy O’Neil – [The era of blind faith in big data must end (TED)](https://www.youtube.com/watch?v=_2u_eHHzRto)  
- Cathy O’Neil – [Weapons of Math Destruction (Talks at Google)](https://youtu.be/TQHs8SA1qpk)  
- Timnit Gebru – [How To Stop Artificial Intelligence From Marginalizing Communities? (TEDxCollegePark)](https://www.youtube.com/watch?v=PWCtoVt1CJM)

### Metrics, Goodhart, and wrong optimization

- [Goodhart’s Law: Focusing on Metrics That Truly Motivate](https://www.youtube.com/watch?v=g-v5cFdt3k4)  
- [Goodhart’s Law (Explained in 100 Seconds)](https://www.youtube.com/watch?v=vlYBd6vi7eQ)

### Technical debt and long-term systems

- Andrea Laforgia – [Technical Debt is Business Debt: Why Engineers Need to Speak the Language of Business](https://www.youtube.com/watch?v=EO8i6QLokSs)

### AI, jobs, economy, and social change

- LSE Research – [Is AI really taking our jobs? The future of work explained](https://www.youtube.com/watch?v=QqfunA6aSS4)  
- [I Investigated AI’s Real Impact on Jobs (No Hype, Just Facts)](https://www.youtube.com/watch?v=jk9BbGfhOfY)  
- Harvard / HBS – [The AI revolution: Myths, risks, and opportunities (Oren Etzioni)](https://www.youtube.com/watch?v=U05IWKf94sM)

### AI, energy, and ecology

- Carbon Brief – [AI: Five charts that put data-centre energy use and emissions into context](https://www.carbonbrief.org/ai-five-charts-that-put-data-centre-energy-use-and-emissions-into-context/)  
- MIT Sloan – [AI has high data center energy costs — but there are solutions](https://mitsloan.mit.edu/ideas-made-to-matter/ai-has-high-data-center-energy-costs-there-are-solutions)

This section doesn’t aim to close any debate or establish a canon. It’s just a starting point to keep thinking with more context and less noise.

{{< smallnote >}}
**Final note**: This series doesn’t aim to lecture or close discussions. It aims to open them with a bit more care, less hype, and more respect for complexity. If it helped someone pause and think before clicking “ask the AI,” it has done its job.
{{< /smallnote >}}
