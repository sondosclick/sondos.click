---
layout: post
title: "AI doesn’t understand (and why we keep obeying it)"
date: 2026-02-08
categories: [AI, Technology, Culture]
tags: [AI, Critical Thinking, Complexity, Authority]
description: "Why the confidence in AI answers doesn’t imply understanding—and how that leads us to obey it."
featureimage: "/images/posts/2026-02-08-la-ia-no-entiende/feature.png"
---

# AI doesn’t understand (and why we keep obeying it)

![Illustration about confident answers without understanding](/images/posts/2026-02-08-la-ia-no-entiende/feature.png)

If there’s one thing we’ve learned quickly about AI, it’s that it **answers with a lot of confidence**.  
It doesn’t hesitate, doesn’t stutter, and rarely says “I don’t know” in an awkward way. It responds with well-structured sentences, tidy arguments, and a tone that inspires trust.

It’s the **black box with a smile** at full power.

And that’s where the problem begins.

Because one thing is for a system to generate convincing answers, and another very different thing is to **understand** what it’s saying. Confusing the two is convenient. **It’s also dangerous.**

For years, we’ve associated authority with language.  
Whoever explains well, articulates ideas clearly, answers quickly… tends to seem competent. AI plays exactly with that human heuristic. It speaks well, so it must know what it’s talking about. It smiles—metaphorically—so we can trust it.

But the box doesn’t understand.

It doesn’t understand context in the human sense.  
It doesn’t understand intention.  
It doesn’t understand consequences.  
It doesn’t understand responsibility.

The box only calculates which response is most probable given some inputs. Nothing more. Nothing less.

![Illustration about delegating decisions to the box](/images/posts/2026-02-08-la-ia-no-entiende/scene-1.png)

And yet, every day, we ask it for decisions.

Let’s imagine another completely fictional scene.

A team has to make a relevant technical decision. There are several options, all with different tradeoffs. Someone proposes:

> “Let’s ask the AI.”

The box replies with a clear comparison, nicely written, with reasonable pros and cons. Nobody objects too much. After all, it sounds sensible. The recommendation is adopted.

When did the key shift happen?

Not when the question was asked.  
But when **no one felt responsible for contradicting the answer**.

The box didn’t decide.  
But everyone acted as if it had.

![Illustration about the neutrality illusion](/images/posts/2026-02-08-la-ia-no-entiende/scene-2.png)

Here a particularly dangerous illusion appears: **neutrality**.

Because the box has no visible interests, we assume it’s objective. Because it has no emotions, we assume it doesn’t make biased mistakes. Because it has no ego, we assume it doesn’t impose criteria.

None of that is true.

The box carries:
- the biases of the data it was trained on,
- the implicit priorities of its designers,
- and the limitations of a system that optimizes probability, not truth.

But because it doesn’t get angry, doesn’t argue, and doesn’t raise its voice, we grant it an authority we rarely give to a human.

The problem isn’t that AI doesn’t understand.  
The problem is that **we act as if it does**.

And that shifts the decision dynamic.

When a person makes a mistake, we look for causes.  
When the box makes a mistake, we look for exceptions.  
When a person hesitates, we question them.  
When the box answers confidently, we accept it.

Little by little, human judgment stops being the center and becomes a superficial validation mechanism. We no longer think to decide; we think to check that the box “hasn’t said anything outrageous.”

And sometimes, not even that.

![Illustration about judgment becoming superficial validation](/images/posts/2026-02-08-la-ia-no-entiende/scene-3.png)

This shift has consequences that go beyond the technical.

At an organizational level, it dilutes responsibility.  
At a social level, it normalizes opaque decisions.  
At an ethical level, it turns “why?” into “who cares?”

Because the box can’t explain why it decided something. It can only explain *how an explanation sounds*.

And when we start accepting explanations without understanding, we start accepting decisions without responsibility.

Maybe that’s why the message *“ask the AI”* is so tempting.  
It doesn’t just promise efficiency. It promises relief. Relief from thinking, from debating, from carrying the weight of deciding.

The box smiles, answers, and lets us move on without friction.

![Illustration about the relief of obeying the box](/images/posts/2026-02-08-la-ia-no-entiende/scene-4.png)

But every time we obey without understanding, we train something more dangerous than a model:  
we train our own **surrender of judgment**.

AI doesn’t understand.  
It never will in the sense we need to delegate authority to it.

The uncomfortable question isn’t when it will understand.  
The question is why **it’s already so easy for us to obey it**.

---

{{< smallnote >}}
**Note**: This article is part of a series that explores how excessive simplification around AI is displacing complexity, judgment, and responsibility. We started with “ask the AI.” Now we follow the authority we’re granting it. What comes next—biases, ethics, and real impact—is a direct consequence.
{{< /smallnote >}}
