---
layout: post
title: "Juniors, learning, and the risk of a generation that doesn’t understand what works"
date: 2026-03-01
categories: [AI, Technology, Culture]
tags: [AI, Learning, Talent, Complexity]
description: "The risk of a generation learning to execute without understanding in the AI era."
featureimage: "/images/posts/2026-03-01-juniors-aprendizaje-y-riesgo/feature.svg"
---

# Juniors, learning, and the risk of a generation that doesn’t understand what works

![Illustration about layered learning](/images/posts/2026-03-01-juniors-aprendizaje-y-riesgo/feature.svg)

Every generation learns with the tools at hand.  
We learned with search engines, forums, incomplete docs, and answers that started with “this isn’t best practice, but…”.

None of that was comfortable.  
But there was an unwritten rule that almost always held:  
if something worked, sooner or later you had to understand why.

With AI, that rule is starting to crack.

Today many juniors don’t search. They ask.  
They don’t investigate. They request.  
They don’t test hypotheses. They wait for answers.

The black box with a smile is always available, never tired, never judgmental, and answers with enviable confidence. You ask it something and it gives you code, explanations, and even nicely written comments. Sometimes it even adds emojis, in case you doubted its good intentions.

And, to the discomfort of many seniors, **a lot of the time it works**.

It’s worth clarifying something from the start to avoid misunderstandings:  
juniors **are not innocent victims** of this situation.

AI doesn’t force them to accept answers without questioning them.  
It doesn’t prevent them from opening the official documentation.  
It doesn’t forbid them from stopping to understand.

Like any professional, they make decisions. And one of those decisions is whether they want to know why something works… or whether it’s enough that it works.

For years we’ve repeated the same mantra about copy/paste:  
*“If you copy code without understanding it, you’ll pay for it sooner or later.”*

AI doesn’t eliminate that problem.  
It makes it more comfortable.

Now the code doesn’t come from a chaotic Stack Overflow thread, but from an elegant, structured, apparently reasoned answer. No more cryptic variable names or comments in Russian. Everything looks clean. Professional. Trustworthy.

And that’s precisely what’s dangerous.

The junior runs the code, passes the basic tests, and sees everything work. They smile. The box smiles too. The commit happens. Progress.

But when someone asks:
> “Why is this done this way?”

The answer is vague.  
Or nonexistent.

Not because the junior isn’t capable, but because **they never walked the mental path that led to that decision**. The box did it for them.

And learning doesn’t happen when something works.  
It happens when you understand why it could have failed.

Here’s a risk we don’t talk about enough: **the atrophy of judgment**.

Judgment isn’t downloadable.  
It doesn’t come in a prompt.  
It doesn’t magically appear after enough iterations with AI.

It’s built by:
- making mistakes,
- investigating causes,
- comparing alternatives,
- and owning the consequences of decisions.

When that process is replaced by constant interaction with the box, the junior learns to *operate the tool*, but not necessarily to *think the system*.

They learn to tune prompts.  
Not to model problems.


Let’s say it plainly:

> ***Knowing how to use AI is not the same as knowing engineering***.

A junior who only knows how to ask the box for things isn’t growing faster. They’re growing differently… and not necessarily in the right direction.

And here organizations can help accelerate the disaster.

If speed is rewarded over understanding, if the implicit message is “as long as it works, go ahead,” it shouldn’t be surprising that juniors optimize exactly for that.

But even in that context, there’s an individual responsibility that can’t be delegated.

Using AI productively as a junior means accepting an uncomfortable truth:

> **If you can’t explain why something works,  
> you don’t understand it yet.**

That doesn’t mean writing everything from scratch or rejecting the tool. It means using it for what it is: an assistant. An accelerator. Not a substitute for thinking.

Ask *what other options exist*.  
Cross-check with official documentation.  
Modify the code and see what breaks.  
Stop to understand before moving on.

AI can accompany that process.  
But it can’t walk it for you.

The underlying problem isn’t that AI “replaces” juniors.  
That’s another oversimplification.

The real risk is to form a generation that’s extremely efficient…  
and dangerously superficial.

Professionals who produce results fast,  
but don’t understand the systems they maintain.  
Who execute well,  
but hesitate when something goes off-script.

A generation that knows how to talk to the box,  
but not how to challenge it.

And when, in a few years, we need seniors with judgment,  
with a memory of past mistakes  
and the ability to decide under uncertainty,  
we’ll discover—too late—  
that nobody became that just by asking well.

---

{{< smallnote >}}
**Note**: This article doesn’t aim to demonize juniors or AI. It aims to remind us that professional learning can’t be automated without a cost. The tool can speed up the path, but it can’t replace it without consequences.
{{< /smallnote >}}
